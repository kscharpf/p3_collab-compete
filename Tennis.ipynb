{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Linux_NoVis/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 Score 0.00 Mean Last 10 0.0000 Mean Last 100 0.0030 Elapsed    121\n",
      "Episode  200 Score 0.00 Mean Last 10 0.0000 Mean Last 100 0.0000 Elapsed    257\n",
      "Episode  300 Score 0.00 Mean Last 10 0.0290 Mean Last 100 0.0029 Elapsed    410\n",
      "Episode  400 Score 0.00 Mean Last 10 0.0000 Mean Last 100 0.0068 Elapsed    571\n",
      "Episode  500 Score 0.00 Mean Last 10 0.0000 Mean Last 100 0.0039 Elapsed    730\n",
      "Episode  600 Score 0.00 Mean Last 10 0.0100 Mean Last 100 0.0030 Elapsed    876\n",
      "Episode  700 Score 0.10 Mean Last 10 0.0100 Mean Last 100 0.0059 Elapsed   1050\n",
      "Episode  800 Score 0.10 Mean Last 10 0.0100 Mean Last 100 0.0206 Elapsed   1246\n",
      "Episode  900 Score 0.09 Mean Last 10 0.0500 Mean Last 100 0.0298 Elapsed   1484\n",
      "Episode 1000 Score 0.00 Mean Last 10 0.0600 Mean Last 100 0.0391 Elapsed   1738\n",
      "Episode 1100 Score 0.10 Mean Last 10 0.0670 Mean Last 100 0.0416 Elapsed   1993\n",
      "Episode 1200 Score 0.10 Mean Last 10 0.1060 Mean Last 100 0.0591 Elapsed   2267\n",
      "Episode 1300 Score 0.09 Mean Last 10 0.0570 Mean Last 100 0.0578 Elapsed   2551\n",
      "Episode 1400 Score 0.00 Mean Last 10 0.0680 Mean Last 100 0.0740 Elapsed   2889\n",
      "Episode 1500 Score 0.10 Mean Last 10 0.0690 Mean Last 100 0.1006 Elapsed   3341\n",
      "Episode 1600 Score 0.10 Mean Last 10 0.1380 Mean Last 100 0.1058 Elapsed   3814\n",
      "Episode 1700 Score 0.10 Mean Last 10 0.1000 Mean Last 100 0.1115 Elapsed   4291\n",
      "Episode 1800 Score 0.10 Mean Last 10 0.1100 Mean Last 100 0.1147 Elapsed   4738\n",
      "Episode 1900 Score 0.30 Mean Last 10 0.0890 Mean Last 100 0.1267 Elapsed   5256\n",
      "Episode 2000 Score 0.00 Mean Last 10 0.2300 Mean Last 100 0.2136 Elapsed   6163\n",
      "Episode 2100 Score 0.30 Mean Last 10 0.2600 Mean Last 100 0.3027 Elapsed   7519\n",
      "Episode 2200 Score 0.10 Mean Last 10 0.2800 Mean Last 100 0.3078 Elapsed   8857\n",
      "Episode 2300 Score 0.30 Mean Last 10 0.3090 Mean Last 100 0.3206 Elapsed  10266\n",
      "Episode 2376 Score 1.30 Mean Last 10 0.5600 Mean Last 100 0.4918 Elapsed  11962Solved in 2377 episodes with mean score: 0.5018\n",
      "Episode 2379 Score 1.70 Mean Last 10 0.6500 Mean Last 100 0.4978 Elapsed  12050Solved in 2380 episodes with mean score: 0.5138\n",
      "Episode 2383 Score 1.80 Mean Last 10 0.5490 Mean Last 100 0.4957 Elapsed  12160Solved in 2384 episodes with mean score: 0.5107\n",
      "Episode 2384 Score 0.20 Mean Last 10 0.7290 Mean Last 100 0.5107 Elapsed  12167Solved in 2385 episodes with mean score: 0.5107\n",
      "Episode 2385 Score 0.80 Mean Last 10 0.7390 Mean Last 100 0.5107 Elapsed  12198Solved in 2386 episodes with mean score: 0.5177\n",
      "Episode 2386 Score 0.30 Mean Last 10 0.7590 Mean Last 100 0.5177 Elapsed  12210Solved in 2387 episodes with mean score: 0.5197\n",
      "Episode 2387 Score 1.10 Mean Last 10 0.6590 Mean Last 100 0.5197 Elapsed  12256Solved in 2388 episodes with mean score: 0.5297\n",
      "Episode 2388 Score 1.50 Mean Last 10 0.7390 Mean Last 100 0.5297 Elapsed  12322Solved in 2389 episodes with mean score: 0.5327\n",
      "Episode 2389 Score 0.60 Mean Last 10 0.8590 Mean Last 100 0.5327 Elapsed  12346Solved in 2390 episodes with mean score: 0.5357\n",
      "Episode 2390 Score 0.30 Mean Last 10 0.7490 Mean Last 100 0.5357 Elapsed  12358Solved in 2391 episodes with mean score: 0.5377\n",
      "Episode 2391 Score 0.40 Mean Last 10 0.7790 Mean Last 100 0.5377 Elapsed  12372Solved in 2392 episodes with mean score: 0.5417\n",
      "Episode 2392 Score 0.39 Mean Last 10 0.8090 Mean Last 100 0.5417 Elapsed  12386Solved in 2393 episodes with mean score: 0.5396\n",
      "Episode 2393 Score 0.80 Mean Last 10 0.7390 Mean Last 100 0.5396 Elapsed  12420Solved in 2394 episodes with mean score: 0.5376\n",
      "Episode 2394 Score 2.10 Mean Last 10 0.6390 Mean Last 100 0.5376 Elapsed  12499Solved in 2395 episodes with mean score: 0.5547\n",
      "Episode 2395 Score 0.20 Mean Last 10 0.8290 Mean Last 100 0.5547 Elapsed  12506Solved in 2396 episodes with mean score: 0.5567\n",
      "Episode 2396 Score 1.80 Mean Last 10 0.7690 Mean Last 100 0.5567 Elapsed  12571Solved in 2397 episodes with mean score: 0.5667\n",
      "Episode 2397 Score 1.40 Mean Last 10 0.9190 Mean Last 100 0.5667 Elapsed  12626Solved in 2398 episodes with mean score: 0.5797\n",
      "Episode 2398 Score 0.30 Mean Last 10 0.9490 Mean Last 100 0.5797 Elapsed  12636Solved in 2399 episodes with mean score: 0.5827\n",
      "Episode 2399 Score 0.30 Mean Last 10 0.8290 Mean Last 100 0.5827 Elapsed  12648Solved in 2400 episodes with mean score: 0.5847\n",
      "Episode 2400 Score 1.60 Mean Last 10 0.7990 Mean Last 100 0.5847 Elapsed  12707\n",
      "Solved in 2401 episodes with mean score: 0.5977\n",
      "Episode 2401 Score 0.80 Mean Last 10 0.9290 Mean Last 100 0.5977 Elapsed  12735Solved in 2402 episodes with mean score: 0.5967\n",
      "Episode 2402 Score 0.00 Mean Last 10 0.9690 Mean Last 100 0.5967 Elapsed  12737Solved in 2403 episodes with mean score: 0.5907\n",
      "Episode 2403 Score 0.30 Mean Last 10 0.9300 Mean Last 100 0.5907 Elapsed  12747Solved in 2404 episodes with mean score: 0.5917\n",
      "Episode 2404 Score 0.10 Mean Last 10 0.8800 Mean Last 100 0.5917 Elapsed  12752Solved in 2405 episodes with mean score: 0.5817\n",
      "Episode 2405 Score 0.40 Mean Last 10 0.6800 Mean Last 100 0.5817 Elapsed  12767Solved in 2406 episodes with mean score: 0.5677\n",
      "Episode 2406 Score 0.50 Mean Last 10 0.7000 Mean Last 100 0.5677 Elapsed  12788Solved in 2407 episodes with mean score: 0.5727\n",
      "Episode 2407 Score 0.80 Mean Last 10 0.5700 Mean Last 100 0.5727 Elapsed  12819Solved in 2408 episodes with mean score: 0.5757\n",
      "Episode 2408 Score 1.70 Mean Last 10 0.5100 Mean Last 100 0.5757 Elapsed  12890Solved in 2409 episodes with mean score: 0.5767\n",
      "Episode 2409 Score 0.80 Mean Last 10 0.6500 Mean Last 100 0.5767 Elapsed  12923Solved in 2410 episodes with mean score: 0.5807\n",
      "Episode 2410 Score 0.80 Mean Last 10 0.7000 Mean Last 100 0.5807 Elapsed  12955Solved in 2411 episodes with mean score: 0.5797\n",
      "Episode 2411 Score 1.00 Mean Last 10 0.6200 Mean Last 100 0.5797 Elapsed  12995Solved in 2412 episodes with mean score: 0.5887\n",
      "Episode 2412 Score 0.50 Mean Last 10 0.6400 Mean Last 100 0.5887 Elapsed  13018Solved in 2413 episodes with mean score: 0.5937\n",
      "Episode 2413 Score 0.50 Mean Last 10 0.6900 Mean Last 100 0.5937 Elapsed  13038Solved in 2414 episodes with mean score: 0.5887\n",
      "Episode 2414 Score 0.20 Mean Last 10 0.7100 Mean Last 100 0.5887 Elapsed  13047Solved in 2415 episodes with mean score: 0.5877\n",
      "Episode 2415 Score 0.00 Mean Last 10 0.7200 Mean Last 100 0.5877 Elapsed  13048Solved in 2416 episodes with mean score: 0.5867\n",
      "Episode 2416 Score 0.20 Mean Last 10 0.6800 Mean Last 100 0.5867 Elapsed  13061Solved in 2417 episodes with mean score: 0.5777\n",
      "Episode 2417 Score 0.50 Mean Last 10 0.6500 Mean Last 100 0.5777 Elapsed  13083Solved in 2418 episodes with mean score: 0.5817\n",
      "Episode 2418 Score 0.20 Mean Last 10 0.6200 Mean Last 100 0.5817 Elapsed  13092Solved in 2419 episodes with mean score: 0.5827\n",
      "Episode 2419 Score 0.10 Mean Last 10 0.4700 Mean Last 100 0.5827 Elapsed  13097Solved in 2420 episodes with mean score: 0.5777\n",
      "Episode 2420 Score 2.30 Mean Last 10 0.4000 Mean Last 100 0.5777 Elapsed  13184Solved in 2421 episodes with mean score: 0.5997\n",
      "Episode 2421 Score 0.70 Mean Last 10 0.5500 Mean Last 100 0.5997 Elapsed  13211Solved in 2422 episodes with mean score: 0.6057\n",
      "Episode 2422 Score 1.40 Mean Last 10 0.5200 Mean Last 100 0.6057 Elapsed  13270Solved in 2423 episodes with mean score: 0.6157\n",
      "Episode 2423 Score 0.30 Mean Last 10 0.6100 Mean Last 100 0.6157 Elapsed  13283Solved in 2424 episodes with mean score: 0.6187\n",
      "Episode 2424 Score 1.49 Mean Last 10 0.5900 Mean Last 100 0.6187 Elapsed  13345Solved in 2425 episodes with mean score: 0.6276\n",
      "Episode 2425 Score 0.10 Mean Last 10 0.7190 Mean Last 100 0.6276 Elapsed  13348Solved in 2426 episodes with mean score: 0.6286\n",
      "Episode 2426 Score 0.10 Mean Last 10 0.7290 Mean Last 100 0.6286 Elapsed  13352Solved in 2427 episodes with mean score: 0.6247\n",
      "Episode 2427 Score 0.50 Mean Last 10 0.7190 Mean Last 100 0.6247 Elapsed  13374Solved in 2428 episodes with mean score: 0.6267\n",
      "Episode 2428 Score 0.10 Mean Last 10 0.7190 Mean Last 100 0.6267 Elapsed  13377Solved in 2429 episodes with mean score: 0.6267\n",
      "Episode 2429 Score 1.00 Mean Last 10 0.7090 Mean Last 100 0.6267 Elapsed  13419Solved in 2430 episodes with mean score: 0.6367\n",
      "Episode 2430 Score 1.50 Mean Last 10 0.7990 Mean Last 100 0.6367 Elapsed  13473Solved in 2431 episodes with mean score: 0.6517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2431 Score 2.60 Mean Last 10 0.7190 Mean Last 100 0.6517 Elapsed  13579Solved in 2432 episodes with mean score: 0.6737\n",
      "Episode 2432 Score 0.40 Mean Last 10 0.9090 Mean Last 100 0.6737 Elapsed  13594Solved in 2433 episodes with mean score: 0.6727\n",
      "Episode 2433 Score 0.70 Mean Last 10 0.8090 Mean Last 100 0.6727 Elapsed  13619Solved in 2434 episodes with mean score: 0.6687\n",
      "Episode 2434 Score 0.70 Mean Last 10 0.8490 Mean Last 100 0.6687 Elapsed  13644Solved in 2435 episodes with mean score: 0.6757\n",
      "Episode 2435 Score 0.40 Mean Last 10 0.7700 Mean Last 100 0.6757 Elapsed  13659Solved in 2436 episodes with mean score: 0.6757\n",
      "Episode 2436 Score 0.10 Mean Last 10 0.8000 Mean Last 100 0.6757 Elapsed  13665Solved in 2437 episodes with mean score: 0.6757\n",
      "Episode 2437 Score 0.70 Mean Last 10 0.8000 Mean Last 100 0.6757 Elapsed  13695Solved in 2438 episodes with mean score: 0.6807\n",
      "Episode 2438 Score 0.30 Mean Last 10 0.8200 Mean Last 100 0.6807 Elapsed  13708Solved in 2439 episodes with mean score: 0.6807\n",
      "Episode 2439 Score 0.30 Mean Last 10 0.8400 Mean Last 100 0.6807 Elapsed  13719Solved in 2440 episodes with mean score: 0.6777\n",
      "Episode 2440 Score 1.90 Mean Last 10 0.7700 Mean Last 100 0.6777 Elapsed  13796Solved in 2441 episodes with mean score: 0.6897\n",
      "Episode 2441 Score 0.30 Mean Last 10 0.8100 Mean Last 100 0.6897 Elapsed  13807Solved in 2442 episodes with mean score: 0.6897\n",
      "Episode 2442 Score 2.20 Mean Last 10 0.5800 Mean Last 100 0.6897 Elapsed  13900Solved in 2443 episodes with mean score: 0.7107\n",
      "Episode 2443 Score 2.20 Mean Last 10 0.7600 Mean Last 100 0.7107 Elapsed  13991Solved in 2444 episodes with mean score: 0.7307\n",
      "Episode 2444 Score 0.09 Mean Last 10 0.9100 Mean Last 100 0.7307 Elapsed  13995Solved in 2445 episodes with mean score: 0.7306\n",
      "Episode 2445 Score 0.70 Mean Last 10 0.8490 Mean Last 100 0.7306 Elapsed  14025Solved in 2446 episodes with mean score: 0.7336\n",
      "Episode 2446 Score 1.70 Mean Last 10 0.8790 Mean Last 100 0.7336 Elapsed  14099Solved in 2447 episodes with mean score: 0.7406\n",
      "Episode 2447 Score 0.50 Mean Last 10 1.0390 Mean Last 100 0.7406 Elapsed  14120Solved in 2448 episodes with mean score: 0.7386\n",
      "Episode 2448 Score 0.50 Mean Last 10 1.0190 Mean Last 100 0.7386 Elapsed  14142Solved in 2449 episodes with mean score: 0.7286\n",
      "Episode 2449 Score 1.10 Mean Last 10 1.0390 Mean Last 100 0.7286 Elapsed  14190Solved in 2450 episodes with mean score: 0.7236\n",
      "Episode 2450 Score 1.70 Mean Last 10 1.1190 Mean Last 100 0.7236 Elapsed  14262Solved in 2451 episodes with mean score: 0.7396\n",
      "Episode 2451 Score 0.80 Mean Last 10 1.0990 Mean Last 100 0.7396 Elapsed  14293Solved in 2452 episodes with mean score: 0.7466\n",
      "Episode 2452 Score 0.00 Mean Last 10 1.1490 Mean Last 100 0.7466 Elapsed  14295Solved in 2453 episodes with mean score: 0.7386\n",
      "Episode 2453 Score 1.30 Mean Last 10 0.9290 Mean Last 100 0.7386 Elapsed  14343Solved in 2454 episodes with mean score: 0.7496\n",
      "Episode 2454 Score 0.50 Mean Last 10 0.8390 Mean Last 100 0.7496 Elapsed  14362Solved in 2455 episodes with mean score: 0.7396\n",
      "Episode 2455 Score 1.40 Mean Last 10 0.8800 Mean Last 100 0.7396 Elapsed  14427Solved in 2456 episodes with mean score: 0.7506\n",
      "Episode 2456 Score 0.90 Mean Last 10 0.9500 Mean Last 100 0.7506 Elapsed  14467Solved in 2457 episodes with mean score: 0.7516\n",
      "Episode 2457 Score 0.00 Mean Last 10 0.8700 Mean Last 100 0.7516 Elapsed  14469Solved in 2458 episodes with mean score: 0.7426\n",
      "Episode 2458 Score 0.10 Mean Last 10 0.8200 Mean Last 100 0.7426 Elapsed  14474Solved in 2459 episodes with mean score: 0.7396\n",
      "Episode 2459 Score 0.70 Mean Last 10 0.7800 Mean Last 100 0.7396 Elapsed  14502Solved in 2460 episodes with mean score: 0.7216\n",
      "Episode 2460 Score 0.59 Mean Last 10 0.7400 Mean Last 100 0.7216 Elapsed  14526Solved in 2461 episodes with mean score: 0.7235\n",
      "Episode 2461 Score 0.10 Mean Last 10 0.6290 Mean Last 100 0.7235 Elapsed  14531Solved in 2462 episodes with mean score: 0.7195\n",
      "Episode 2462 Score 0.90 Mean Last 10 0.5590 Mean Last 100 0.7195 Elapsed  14566Solved in 2463 episodes with mean score: 0.7275\n",
      "Episode 2463 Score 0.40 Mean Last 10 0.6490 Mean Last 100 0.7275 Elapsed  14580Solved in 2464 episodes with mean score: 0.7235\n",
      "Episode 2464 Score 0.50 Mean Last 10 0.5590 Mean Last 100 0.7235 Elapsed  14598Solved in 2465 episodes with mean score: 0.7255\n",
      "Episode 2465 Score 0.50 Mean Last 10 0.5590 Mean Last 100 0.7255 Elapsed  14617Solved in 2466 episodes with mean score: 0.7295\n",
      "Episode 2466 Score 2.10 Mean Last 10 0.4690 Mean Last 100 0.7295 Elapsed  14704Solved in 2467 episodes with mean score: 0.7505\n",
      "Episode 2467 Score 0.90 Mean Last 10 0.5890 Mean Last 100 0.7505 Elapsed  14746Solved in 2468 episodes with mean score: 0.7505\n",
      "Episode 2468 Score 0.29 Mean Last 10 0.6790 Mean Last 100 0.7505 Elapsed  14758Solved in 2469 episodes with mean score: 0.7524\n",
      "Episode 2469 Score 0.70 Mean Last 10 0.6980 Mean Last 100 0.7524 Elapsed  14788Solved in 2470 episodes with mean score: 0.7574\n",
      "Episode 2470 Score 0.10 Mean Last 10 0.6980 Mean Last 100 0.7574 Elapsed  14791"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-273156fcf35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Send in these two states, actions, next_states, and dones into the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/school/DRL/deep-reinforcement-learning/p3_collab-compete/maddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# for each agent, select a unique mini-batch of tuples and then proceed to learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/school/DRL/deep-reinforcement-learning/p3_collab-compete/maddpg_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mall_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_next_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/school/DRL/deep-reinforcement-learning/p3_collab-compete/ddpg_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, agent_id, experiences, gamma, all_next_actions, all_actions)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from maddpg_agent import MADDPGAgent\n",
    "import time\n",
    "agent = MADDPGAgent(num_agents, state_size, action_size, random_seed=47, batch_size=256, buffer_size=int(1e6), use_batch_norm=True)\n",
    "episode_scores = []\n",
    "best_score=0.\n",
    "start_time = time.time()\n",
    "for i in range(5000):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    agent.reset_noise()\n",
    "    game_time = 0\n",
    "    while True:\n",
    "        # states[0] is the observation of agent 0 and states[1] is the observation of agent 1\n",
    "        # actions[0] is the action chosen by agent 0 and actions[1] is the action chosen by agent 1\n",
    "        # selection of action is always based only on the observation\n",
    "        actions = agent.act(states)\n",
    "\n",
    "        # Apply these actions to the environment and receive the next state, rewards, and doneness\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        \n",
    "        #print(\"actions: {} rewards: {}\".format(actions, rewards))\n",
    "        \n",
    "        dones = env_info.local_done\n",
    "        \n",
    "        # Send in these two states, actions, next_states, and dones into the agent\n",
    "        agent.step(states,actions,rewards,next_states, dones)\n",
    "        scores += np.array(env_info.rewards)\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "        game_time += 1\n",
    "    \n",
    "    mean_last_10 = 0.\n",
    "    mean_last_100 = 0.\n",
    "    episode_scores.append(np.max(scores))\n",
    "\n",
    "    if i >= 10:\n",
    "        mean_last_10 = np.mean(episode_scores[i-10:i])\n",
    "        if i>= 100:\n",
    "            mean_last_100 = np.mean(episode_scores[i-100:i])\n",
    "            if mean_last_100 >= 0.5:\n",
    "                print(\"Solved in {} episodes with mean score: {:.4f}\".format(i, mean_last_100))\n",
    "                break\n",
    "    elapsed_time = int(time.time() - start_time)\n",
    "    sfmt = \"\\rEpisode {:4d} Score {:.2f} Mean Last 10 {:.4f} Mean Last 100 {:.4f} Elapsed {:6d}\".format(i,\n",
    "                                                                                  np.max(scores),\n",
    "                                                                                  mean_last_10,\n",
    "                                                                                  mean_last_100,\n",
    "                                                                                  elapsed_time)\n",
    "    print(sfmt, end=\"\")\n",
    "    if i%100 == 0 and i>0:\n",
    "        print(sfmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(episode_scores, open(\"data/episode_scores.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(agent.agents[0].actor_local.state_dict(),open(\"data/actor_local0.pt\",\"wb\"))\n",
    "torch.save(agent.agents[1].actor_local.state_dict(), open(\"data/actor_local1.pt\",\"wb\"))\n",
    "torch.save(agent.agents[0].critic_local.state_dict(),open(\"data/critic_local0.pt\",\"wb\"))\n",
    "torch.save(agent.agents[1].critic_local.state_dict(), open(\"data/critic_local1.pt\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 508.875x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = {'raw scores':episode_scores, 'smoothed x100 scores':[]}\n",
    "d['smoothed x100 scores'] = [np.mean(episode_scores[max(i-100,0):i+1]) for i in range(len(episode_scores))]\n",
    "pdf = pd.DataFrame(d)\n",
    "ax=sns.relplot(data=pdf,kind='line')\n",
    "ax.set(xlabel=\"Episode\",ylabel=\"Score\", title=\"Raw and Smoothed Tennis Scores\")\n",
    "ax.savefig(\"output/scores.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "Game 1 Score 0.0\n",
      "Game 2 Score 0.0\n"
     ]
    }
   ],
   "source": [
    "try: env\n",
    "except: env = None\n",
    "from maddpg_agent import MADDPGAgent\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "if env:\n",
    "    env.close()\n",
    "env = UnityEnvironment(file_name='./Tennis_Linux/Tennis.x86_64')\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "agent = MADDPGAgent(num_agents, state_size, action_size, random_seed=47, batch_size=256, buffer_size=int(1e6), use_batch_norm=True)\n",
    "agent.agents[0].actor_local.load_state_dict(torch.load(\"./data/actor_local0.pt\"))\n",
    "agent.agents[1].actor_local.load_state_dict(torch.load(\"./data/actor_local1.pt\"))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    agent.reset_noise()\n",
    "    game_time = 0\n",
    "    while True:\n",
    "        # states[0] is the observation of agent 0 and states[1] is the observation of agent 1\n",
    "        # actions[0] is the action chosen by agent 0 and actions[1] is the action chosen by agent 1\n",
    "        # selection of action is always based only on the observation\n",
    "        actions = agent.act(states)\n",
    "\n",
    "        # Apply these actions to the environment and receive the next state, rewards, and doneness\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards        \n",
    "        dones = env_info.local_done\n",
    "        \n",
    "        # Send in these two states, actions, next_states, and dones into the agent\n",
    "        scores += np.array(env_info.rewards)\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    print(\"Game {} Score {}\".format(i+1, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
